import feedparser
import csv
import requests
import re
import cloudscraper
import os

SOURCES = [
    "https://nyaa.si/?page=rss&q=Arabic+1080p",
    "https://nyaa.si/?page=rss&q=Arabic+720p",
    "https://nyaa.si/?page=rss&q=Arabic+480p",
    "https://www.tokyotosho.info/rss.php?filter=1,11&z=Arabic"
]

MAX_ROWS = 10000  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ø³Ø·Ø± ÙÙŠ ÙƒÙ„ Ù…Ù„Ù

def get_current_db_file():
    """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¢Ø®Ø± Ù…Ù„Ù Ù…ØªØ§Ø­ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯"""
    i = 0
    while True:
        filename = f'database_{i}.csv' if i > 0 else 'database.csv'
        if not os.path.exists(filename):
            return filename
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø± ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ
        with open(filename, 'r', encoding='utf-8') as f:
            row_count = sum(1 for row in f)
        
        if row_count < MAX_ROWS:
            return filename
        i += 1

def clean_and_translate(text):
    clean_text = re.sub(r'\[.*?\]|\(.*?\)|1080p|720p|480p|HEVC|x264|x265|AAC', '', text).strip()
    try:
        url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=ar&dt=t&q={requests.utils.quote(clean_text)}"
        res = requests.get(url, timeout=5)
        return res.json()[0][0][0]
    except:
        return clean_text

def get_clean_hash_link(entry):
    if hasattr(entry, 'nyaa_infohash'):
        return f"https://webtor.io/player/embed/{entry.nyaa_infohash}"
    link = getattr(entry, 'link', '')
    hash_match = re.search(r'btih:([a-fA-F0-9]{40})', link)
    if hash_match:
        return f"https://webtor.io/player/embed/{hash_match.group(1).lower()}"
    return None

def start_bot():
    scraper = cloudscraper.create_scraper()
    db_file = get_current_db_file()
    print(f"ğŸ“‚ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ø¹Ù…Ù„: {db_file}")

    new_entries = []
    for rss_url in SOURCES:
        try:
            resp = scraper.get(rss_url, timeout=15)
            feed = feedparser.parse(resp.text)
            for entry in feed.entries[:25]:
                streaming_link = get_clean_hash_link(entry)
                if streaming_link:
                    final_name = clean_and_translate(entry.title)
                    quality = "1080p (FHD)" if "1080p" in entry.title else "720p (HD)" if "720p" in entry.title else "480p (SD)"
                    
                    new_entries.append({
                        'name_ar': final_name,
                        'name_en': final_name,
                        'torrent_url': streaming_link,
                        'status': quality
                    })
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£: {e}")

    # Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¨Ù†Ø¸Ø§Ù… Append (Ø§Ù„Ø¥Ø¶Ø§ÙØ©) Ù„Ø¹Ø¯Ù… Ù…Ø³Ø­ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    file_exists = os.path.isfile(db_file)
    with open(db_file, 'a', newline='', encoding='utf-8') as f:
        fieldnames = ['name_ar', 'name_en', 'torrent_url', 'status']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists or os.stat(db_file).st_size == 0:
            writer.writeheader()
        writer.writerows(new_entries)
    
    print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© {len(new_entries)} Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ù„Ù‰ {db_file}")

if __name__ == "__main__":
    start_bot()
