import cloudscraper
from bs4 import BeautifulSoup
import csv
import os
import requests

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ù†Ø§Øµ Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ©
scraper = cloudscraper.create_scraper()

def check_link_status(url):
    """ÙŠÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø§ ÙŠØ²Ø§Ù„ ÙŠØ¹Ù…Ù„"""
    if not url: return False
    try:
        # Ø¨Ø¹Ø¶ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª ØªÙ…Ù†Ø¹ Ø·Ù„Ø¨Ø§Øª HEADØŒ Ù„Ø°Ø§ Ù†Ø³ØªØ®Ø¯Ù… GET Ù…Ø¹ stream
        response = requests.get(url, timeout=5, stream=True)
        return response.status_code == 200
    except:
        return False

def get_video_links(page_url):
    """ÙŠØ³Ø­Ø¨ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬ÙˆØ¯Ø§Øª Ù…Ù† Ù‚Ù„Ø¨ ØµÙØ­Ø© Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§"""
    links = {"1080p": "", "720p": "", "480p": ""}
    try:
        res = scraper.get(page_url)
        soup = BeautifulSoup(res.content, 'html.parser')
        
        # Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§ ÙŠØ¶Ø¹ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª ØºØ§Ù„Ø¨Ø§Ù‹ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© 'WatchServersList' Ø£Ùˆ Ø¯Ø§Ø®Ù„ Ø£Ø²Ø±Ø§Ø±
        servers = soup.find_all('btn', {'data-url': True}) or soup.find_all('iframe', src=True)
        
        for s in servers:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¬Ù„Ø¨ Ø§Ù„Ø±Ø§Ø¨Ø· Ø³ÙˆØ§Ø¡ ÙƒØ§Ù† ÙÙŠ data-url Ø£Ùˆ src
            url = s.get('data-url') or s.get('src')
            if not url: continue
            if url.startswith('//'): url = 'https:' + url
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Øµ Ø§Ù„Ø²Ø± Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø·
            label = s.text.lower()
            if "1080" in label or "fhd" in url: links["1080p"] = url
            elif "720" in label or "hd" in url: links["720p"] = url
            elif "480" in label or "sd" in url: links["480p"] = url
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ø¬ÙˆØ¯Ø© Ù…Ø¹ÙŠÙ†Ø©ØŒ Ù†Ø¶Ø¹ Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø· Ù†Ø¯Ù‡ ÙƒØ¬ÙˆØ¯Ø© Ø£Ø³Ø§Ø³ÙŠØ© 720p
        if not links["720p"] and servers:
            links["720p"] = servers[0].get('data-url') or servers[0].get('src')
            
        return links
    except:
        return links

def update_database():
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø³ØªÙ‡Ø¯Ù (Ù‚Ø³Ù… Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªØ±ÙƒÙŠØ© ÙÙŠ Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§)
    source_url = "https://mycima.gold/category/series/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%aa%d8%b1%d9%83%d9%8a%d8%a9/"
    db_file = 'database.csv'
    temp_data = []

    # 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„ÙØ­ØµÙ‡Ø§ ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´ØºØ§Ù„Ø©
    if os.path.exists(db_file):
        with open(db_file, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø§Ø¨Ø·Ù‡Ø§ Ù„Ø§ ÙŠØ²Ø§Ù„ ÙŠØ¹Ù…Ù„
                if check_link_status(row.get('url_720p')):
                    temp_data.append(row)

    # 2. Ø³Ø­Ø¨ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹
    print(f"ğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ: {source_url}")
    try:
        res = scraper.get(source_url)
        soup = BeautifulSoup(res.content, 'html.parser')
        
        # ØªØ­Ø¯ÙŠØ¯ Ø­Ø§ÙˆÙŠØ© Ø§Ù„Ø­Ù„Ù‚Ø§Øª ÙÙŠ Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§ (ØºØ§Ù„Ø¨Ø§Ù‹ GridItem)
        items = soup.find_all('div', class_='GridItem')

        for item in items[:15]: # ÙØ­Øµ Ø¢Ø®Ø± 15 Ø­Ù„Ù‚Ø© Ù…Ø¶Ø§ÙØ©
            title_tag = item.find('strong') or item.find('h2')
            link_tag = item.find('a', href=True)
            
            if not title_tag or not link_tag: continue
            
            name = title_tag.text.strip()
            link = link_tag['href']
            
            # Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø­Ù„Ù‚Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŒ ØªØ®Ø·Ø§Ù‡Ø§
            if any(d['name'] == name for d in temp_data):
                continue
            
            print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø±ÙˆØ§Ø¨Ø· Ù‚Ù„Ø¨ Ø§Ù„Ø­Ù„Ù‚Ø©: {name}")
            v_links = get_video_links(link)
            
            temp_data.append({
                'name': name,
                'url_1080p': v_links['1080p'],
                'url_720p': v_links['720p'],
                'url_480p': v_links['480p']
            })
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³Ø­Ø¨: {e}")

    # 3. Ø­ÙØ¸ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø§Ù„Ø§Ø³Ù… Ø«Ù… Ø§Ù„Ø±ÙˆØ§Ø¨Ø·) Ø¨Ø´ÙƒÙ„ Ù…Ø±ØªØ¨
    with open(db_file, mode='w', newline='', encoding='utf-8') as f:
        fieldnames = ['name', 'url_1080p', 'url_720p', 'url_480p']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(temp_data)
    
    print(f"âœ… ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨Ù†Ø¬Ø§Ø­. Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø§Øª ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„: {len(temp_data)}")

if __name__ == "__main__":
    update_database()
