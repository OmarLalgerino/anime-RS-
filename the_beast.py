import requests
import csv
import re
import cloudscraper
import os
from bs4 import BeautifulSoup

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
SOURCES = [
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/ar.m3u",
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/s.m3u"
]
EGYDEAD_URL = "https://egydead.rip/episode/a-knight-of-the-seven-kingdoms-s01e03/"
SPORTS_KEYWORDS = ['sport', 'beIN', 'SSC', 'KSA', 'Ø±ÙŠØ§Ø¶Ø©']
DB_FILE = 'database.csv'

def check_link(url):
    """ÙØ­Øµ Ù‡Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„ ÙˆÙŠØ±Ø³Ù„ Ø¨ÙŠØ§Ù†Ø§ØªØŸ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://egydead.rip/'
        }
        # Ù†Ø³ØªØ®Ø¯Ù… HEAD Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙØ­Øµ Ø¯ÙˆÙ† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„Ø§Ù‹
        with requests.head(url, timeout=5, headers=headers, allow_redirects=True) as r:
            return r.status_code == 200
    except:
        return False

def extract_egydead_servers(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù† ØµÙØ­Ø© Egydead"""
    print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù†: {url}")
    scraper = cloudscraper.create_scraper()
    servers = []
    try:
        response = scraper.get(url, timeout=15)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ÙÙŠ iframes Ø£Ùˆ Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª)
        # Ù†Ø¨Ø­Ø« Ø¹Ù† ÙˆØ³ÙˆÙ… Ø§Ù„Ù€ iframe Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø´ØºÙ„Ø§Øª ÙÙŠØ¯ÙŠÙˆ
        iframes = soup.find_all('iframe')
        for iframe in iframes:
            src = iframe.get('src') or iframe.get('data-src')
            if src and ('http' in src):
                servers.append({'title': 'EgyDead Server (Iframe)', 'url': src})
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‚Ø§Ø¦Ù…Ø©
        server_list = soup.select('ul.servers-list li') # Ù…Ø«Ø§Ù„ Ù„Ù…Ø­Ø¯Ø¯ Ø§ÙØªØ±Ø§Ø¶ÙŠ
        for s in server_list:
            link = s.get('data-url')
            if link:
                servers.append({'title': f'EgyDead - {s.text.strip()}', 'url': link})
                
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙƒØ´Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹: {e}")
    return servers

def is_token_link(url):
    token_patterns = ['token=', 'key=', 'auth', 'pass', 'user']
    if any(p in url.lower() for p in token_patterns): return True
    return any(len(segment) > 25 for segment in url.split('/'))

def start_process():
    scraper = cloudscraper.create_scraper()
    final_list = []
    seen_urls = set()

    # 1. ÙØ­Øµ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    if os.path.exists(DB_FILE):
        print("ğŸ” ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹...")
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if check_link(row['url']):
                    final_list.append(row)
                    seen_urls.add(row['url'])

    # 2. Ø³Ø­Ø¨ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† ØµÙØ­Ø© Egydead Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
    egy_servers = extract_egydead_servers(EGYDEAD_URL)
    for srv in egy_servers:
        if srv['url'] not in seen_urls:
            # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ù„Ø§ ØªØ¹Ø·ÙŠ 200 HEAD Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø£Ù†Ù‡Ø§ ØµÙØ­Ø§Øª HTML
            # Ù„Ø°Ø§ Ø³Ù†Ø¶ÙŠÙÙ‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø£Ùˆ Ù†ÙØ­ØµÙ‡Ø§ Ø¨Ù€ GET
            final_list.append(srv)
            seen_urls.add(srv['url'])
            print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø³ÙŠØ±ÙØ± Ù…Ø´Ø§Ù‡Ø¯Ø©: {srv['title']}")

    # 3. Ø¬Ù„Ø¨ Ù‚Ù†ÙˆØ§Øª IPTV Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©
    print("ğŸ“¡ Ø¬Ù„Ø¨ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ù…Ù† GitHub...")
    for source in SOURCES:
        try:
            response = scraper.get(source, timeout=15)
            matches = re.findall(r'#EXTINF:.*?,(.*?)\n(http.*?)\n', response.text)
            for name, url in matches:
                url = url.strip()
                name = name.strip()
                if any(key in name.lower() for key in SPORTS_KEYWORDS):
                    if not is_token_link(url) and url not in seen_urls:
                        if check_link(url):
                            final_list.append({'title': name, 'url': url})
                            seen_urls.add(url)
                            print(f"âœ… Ø¥Ø¶Ø§ÙØ© Ù‚Ù†Ø§Ø© Ø±ÙŠØ§Ø¶ÙŠØ©: {name}")
        except: continue

    # 4. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ù„Ù
    with open(DB_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'url'])
        writer.writeheader()
        writer.writerows(final_list)
    
    print(f"\nâœ¨ Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«! Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(final_list)} Ø±Ø§Ø¨Ø· Ø´ØºÙ‘Ø§Ù„.")

if __name__ == "__main__":
    start_process()
