import cloudscraper
from bs4 import BeautifulSoup
import csv
import re

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù‚Ù†Ø§Øµ Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
scraper = cloudscraper.create_scraper()

def get_video_links(page_url):
    """Ø³Ø­Ø¨ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬ÙˆØ¯Ø§Øª Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ø­Ù„Ù‚Ø© ÙÙŠ Ø¹Ø±Ø¨ Ø³ÙŠØ¯"""
    links = {"1080p": "", "720p": "", "480p": ""}
    try:
        res = scraper.get(page_url, timeout=10)
        soup = BeautifulSoup(res.content, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØµÙØ­Ø©
        all_a = soup.find_all('a', href=True)
        for a in all_a:
            href = a['href']
            text = a.text.lower()
            
            # ØµÙŠØ¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© (mp4, mkv, m3u8)
            if any(ext in href for ext in ['.mp4', '.mkv', '.m3u8']):
                if "1080" in text or "1080" in href: 
                    if not links["1080p"]: links["1080p"] = href
                elif "720" in text or "720" in href: 
                    if not links["720p"]: links["720p"] = href
                elif "480" in text or "480" in href: 
                    if not links["480p"]: links["480p"] = href
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ ÙŠØ¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´ØºÙ„ (Iframe)
        if not links["720p"]:
            iframe = soup.find('iframe', src=True)
            if iframe:
                src = iframe['src']
                links["720p"] = src if src.startswith('http') else 'https:' + src
                
        return links
    except:
        return links

def update_database():
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø°ÙŠ Ø²ÙˆØ¯ØªÙ†ÙŠ Ø¨Ù‡ (Ù‚Ø³Ù… Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªØ±ÙƒÙŠØ©)
    source_url = "https://asd.pics/home3/category/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%aa%d8%b1%d9%83%d9%8a%d8%a9/"
    db_file = 'database.csv'
    all_data = []

    print(f"ğŸš€ Ø§Ù†Ø·Ù„Ø§Ù‚ Ø§Ù„ÙˆØ­Ø´ Ù†Ø­Ùˆ: {source_url}")
    try:
        res = scraper.get(source_url)
        soup = BeautifulSoup(res.content, 'html.parser')
        
        # ÙÙŠ Ø¹Ø±Ø¨ Ø³ÙŠØ¯ØŒ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª ØªÙƒÙˆÙ† Ø¯Ø§Ø®Ù„ div Ø¨Ù€ class MovieBlock
        items = soup.find_all('div', class_='MovieBlock')

        if not items:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ù„Ù‚Ø§ØªØŒ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„ÙƒÙ„Ø§Ø³ Ù‚Ø¯ ØªØºÙŠØ±.")
        
        for item in items[:20]: # Ø³Ø­Ø¨ Ø¢Ø®Ø± 20 Ø­Ù„Ù‚Ø©
            name_tag = item.find('h2')
            link_tag = item.find('a', href=True)
            
            if name_tag and link_tag:
                name = name_tag.text.strip()
                link = link_tag['href']
                
                print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ Ù‚Ù†Øµ: {name}")
                v_links = get_video_links(link)
                
                all_data.append({
                    'name': name,
                    'url_1080p': v_links['1080p'],
                    'url_720p': v_links['720p'],
                    'url_480p': v_links['480p']
                })

        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù„Ù
        with open(db_file, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'url_1080p', 'url_720p', 'url_480p'])
            writer.writeheader()
            writer.writerows(all_data)
        print(f"âœ… Ù…Ø¨Ø±ÙˆÙƒ! ØªÙ… ØªØ­Ø¯ÙŠØ« {len(all_data)} Ø­Ù„Ù‚Ø© Ø¨Ù†Ø¬Ø§Ø­.")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø³Ø­Ø¨: {e}")

if __name__ == "__main__":
    update_database()
